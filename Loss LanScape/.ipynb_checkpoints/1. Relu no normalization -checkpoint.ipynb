{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T18:43:55.458611Z",
     "start_time": "2019-05-03T18:43:42.557277Z"
    },
    "code_folding": []
   },
   "source": [
    "# ==== Import Library ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:10:27.580112Z",
     "start_time": "2019-05-12T00:10:20.959789Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import library\n",
    "import argparse,copy,h5py, os,sys,time,socket\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch,torchvision,torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from   torchsummary import summary\n",
    "from   torch.autograd.variable import Variable\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker,colors\n",
    "\n",
    "# seed value and plotly\n",
    "init_notebook_mode(connected=True); torch.manual_seed(7); torch.cuda.manual_seed_all(7); np.set_printoptions(suppress=True,precision=3,); tf.set_random_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Set up the Network ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:10:27.656908Z",
     "start_time": "2019-05-12T00:10:27.582107Z"
    },
    "code_folding": [
     33,
     95
    ]
   },
   "outputs": [],
   "source": [
    "# need the module \n",
    "class AllConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
    "        super(AllConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 64, 3, padding=1,bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1,bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1, stride=2,bias=False)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1,bias=False)\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, padding=1,bias=False)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3, padding=1, stride=2,bias=False)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 3, padding=1,bias=False)\n",
    "        self.conv8 = nn.Conv2d(128, 128, 1,bias=False)\n",
    "        self.class_conv = nn.Conv2d(128, n_classes, 1,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "#         conv1_out = F.relu(self.conv1(x))\n",
    "#         conv2_out = F.relu(self.conv2(conv1_out))\n",
    "#         conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        \n",
    "#         conv4_out = F.relu(self.conv4(conv3_out))\n",
    "#         conv5_out = F.relu(self.conv5(conv4_out))\n",
    "#         conv6_out = F.relu(self.conv6(conv5_out))\n",
    "        \n",
    "#         conv7_out = F.relu(self.conv7(conv6_out))\n",
    "#         conv8_out = F.relu(self.conv8(conv7_out))\n",
    "#         class_out = F.relu(self.class_conv(conv8_out))\n",
    "#         pool_out  = F.adaptive_avg_pool2d(class_out, 1)\n",
    "        \n",
    "        conv1_out = F.elu(self.conv1(x))\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv3_out = F.elu(self.conv3(conv2_out))\n",
    "        \n",
    "        conv4_out = self.conv4(conv3_out)\n",
    "        conv5_out = F.elu(self.conv5(conv4_out))\n",
    "        conv6_out = self.conv6(conv5_out)\n",
    "        \n",
    "        conv7_out = F.elu(self.conv7(conv6_out))\n",
    "        conv8_out = self.conv8(conv7_out)\n",
    "        class_out = F.elu(self.class_conv(conv8_out))\n",
    "        pool_out  = F.adaptive_avg_pool2d(class_out, 1)\n",
    "        \n",
    "        pool_out.squeeze_(-1)\n",
    "        pool_out.squeeze_(-1)\n",
    "        return pool_out\n",
    "    \n",
    "    \n",
    "class AllConvNet_BN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
    "        super(AllConvNet_BN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_size,96, 3, padding=1,bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1,bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2,bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(96)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1,bias=False)\n",
    "        self.bn4   = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1,bias=False)\n",
    "        self.bn5   = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2,bias=False)\n",
    "        self.bn6   = nn.BatchNorm2d(192)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1,bias=False)\n",
    "        self.bn7   = nn.BatchNorm2d(192)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(192, 192, 1,bias=False)\n",
    "        self.bn8   = nn.BatchNorm2d(192)\n",
    "        \n",
    "        self.class_conv = nn.Conv2d(192, n_classes, 1,bias=False)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1_out = F.relu(self.conv1(x))\n",
    "        conv1_out = self.bn1(conv1_out)\n",
    "        \n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv2_out = self.bn2(conv2_out)\n",
    "\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        conv3_out = self.bn3(conv3_out)\n",
    "        \n",
    "        conv4_out = F.relu(self.conv4(conv3_out))\n",
    "        conv4_out = self.bn4(conv4_out)\n",
    "\n",
    "        conv5_out = F.relu(self.conv5(conv4_out))\n",
    "        conv5_out = self.bn5(conv5_out)\n",
    "\n",
    "        conv6_out = F.relu(self.conv6(conv5_out))\n",
    "        conv6_out = self.bn6(conv6_out)\n",
    "        \n",
    "        conv7_out = F.relu(self.conv7(conv6_out))\n",
    "        conv7_out = self.bn7(conv7_out)\n",
    "\n",
    "        conv8_out = F.relu(self.conv8(conv7_out))\n",
    "        conv8_out = self.bn8(conv8_out)\n",
    "\n",
    "        class_out = F.relu(self.class_conv(conv8_out))\n",
    "        pool_out  = F.adaptive_avg_pool2d(class_out, 1)\n",
    "        \n",
    "        pool_out.squeeze_(-1)\n",
    "        pool_out.squeeze_(-1)\n",
    "        return pool_out\n",
    "class AllConvNet_LocalRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
    "        super(AllConvNet_BN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_size,96, 3, padding=1,bias=False)\n",
    "        self.bn1   = nn.LocalResponseNorm(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1,bias=False)\n",
    "        self.bn2   = nn.LocalResponseNorm(96)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2,bias=False)\n",
    "        self.bn3   = nn.LocalResponseNorm(96)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1,bias=False)\n",
    "        self.bn4   = nn.LocalResponseNorm(192)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1,bias=False)\n",
    "        self.bn5   = nn.LocalResponseNorm(192)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2,bias=False)\n",
    "        self.bn6   = nn.LocalResponseNorm(192)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1,bias=False)\n",
    "        self.bn7   = nn.LocalResponseNorm(192)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(192, 192, 1,bias=False)\n",
    "        self.bn8   = nn.LocalResponseNorm(192)\n",
    "        \n",
    "        self.class_conv = nn.Conv2d(192, n_classes, 1,bias=False)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1_out = F.relu(self.conv1(x))\n",
    "        conv1_out = self.bn1(conv1_out)\n",
    "        \n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv2_out = self.bn2(conv2_out)\n",
    "\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        conv3_out = self.bn3(conv3_out)\n",
    "        \n",
    "        conv4_out = F.relu(self.conv4(conv3_out))\n",
    "        conv4_out = self.bn4(conv4_out)\n",
    "\n",
    "        conv5_out = F.relu(self.conv5(conv4_out))\n",
    "        conv5_out = self.bn5(conv5_out)\n",
    "\n",
    "        conv6_out = F.relu(self.conv6(conv5_out))\n",
    "        conv6_out = self.bn6(conv6_out)\n",
    "        \n",
    "        conv7_out = F.relu(self.conv7(conv6_out))\n",
    "        conv7_out = self.bn7(conv7_out)\n",
    "\n",
    "        conv8_out = F.relu(self.conv8(conv7_out))\n",
    "        conv8_out = self.bn8(conv8_out)\n",
    "\n",
    "        class_out = F.relu(self.class_conv(conv8_out))\n",
    "        pool_out  = F.adaptive_avg_pool2d(class_out, 1)\n",
    "        \n",
    "        pool_out.squeeze_(-1)\n",
    "        pool_out.squeeze_(-1)\n",
    "        return pool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:10:32.732342Z",
     "start_time": "2019-05-12T00:10:27.660897Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "            Conv2d-2           [-1, 64, 32, 32]          36,864\n",
      "            Conv2d-3           [-1, 64, 16, 16]          36,864\n",
      "            Conv2d-4          [-1, 128, 16, 16]          73,728\n",
      "            Conv2d-5          [-1, 128, 16, 16]         147,456\n",
      "            Conv2d-6            [-1, 128, 8, 8]         147,456\n",
      "            Conv2d-7            [-1, 128, 8, 8]         147,456\n",
      "            Conv2d-8            [-1, 128, 8, 8]          16,384\n",
      "            Conv2d-9             [-1, 10, 8, 8]           1,280\n",
      "================================================================\n",
      "Total params: 609,216\n",
      "Trainable params: 609,216\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.82\n",
      "Params size (MB): 2.32\n",
      "Estimated Total Size (MB): 4.15\n",
      "----------------------------------------------------------------\n",
      "Cuda:  True\n"
     ]
    }
   ],
   "source": [
    "# load model with some batch size\n",
    "batch_size = 128 ; learning_rate = 0.0008\n",
    "net        = AllConvNet(3).cuda()\n",
    "\n",
    "# load the loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "net.eval()\n",
    "summary(net, (3, 32, 32))\n",
    "print(\"Cuda: \",next(net.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:10:32.778220Z",
     "start_time": "2019-05-12T00:10:32.740321Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer torch.Size([64, 3, 3, 3])\n",
      "2 layer torch.Size([64, 64, 3, 3])\n",
      "3 layer torch.Size([64, 64, 3, 3])\n",
      "4 layer torch.Size([128, 64, 3, 3])\n",
      "5 layer torch.Size([128, 128, 3, 3])\n",
      "6 layer torch.Size([128, 128, 3, 3])\n",
      "7 layer torch.Size([128, 128, 3, 3])\n",
      "8 layer torch.Size([128, 128, 1, 1])\n",
      "9 layer torch.Size([10, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#  Get the weights - after training\n",
    "def get_weights(net):            return [p.data for p in net.parameters()]\n",
    "def get_random_weights(weights): return [torch.randn(w.size()) for w in weights]\n",
    "def normalize_direction(direction, weights,norm='filter'):\n",
    "    if norm == 'filter':\n",
    "        # Rescale the filters (weights in group) in 'direction' so that each filter has the same norm as its corresponding filter in 'weights'.\n",
    "        for d, w in zip(direction, weights): \n",
    "            # the direction channel gets norm via the channel\n",
    "            d.mul_(w.norm()/(d.norm() + 1e-10))\n",
    "\n",
    "    elif norm == 'layer':\n",
    "        # Rescale the layer variables in the direction so that each layer has the same norm as the layer variables in weights.\n",
    "        direction.mul_(weights.norm()/direction.norm())\n",
    "\n",
    "    elif norm == 'weight':\n",
    "        # Rescale the entries in the direction so that each entry has the same scale as the corresponding weight.\n",
    "        direction.mul_(weights.cpu())\n",
    "\n",
    "    elif norm == 'dfilter':\n",
    "        # Rescale the entries in the direction so that each filter direction has the unit norm.\n",
    "        for d in direction: \n",
    "            d.div_(d.norm() + 1e-10)\n",
    "\n",
    "    elif norm == 'dlayer':\n",
    "        # Rescale the entries in the direction so that each layer direction has the unit norm.\n",
    "        direction.div_(direction.norm())\n",
    "weight      = get_weights(net)\n",
    "temp_layer  = 0\n",
    "for x in weight: \n",
    "    temp_layer = temp_layer + 1\n",
    "    print(str(temp_layer) + \" layer \" + str(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:10:34.987316Z",
     "start_time": "2019-05-12T00:10:32.780216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Split: train\n",
      "    Root Location: .\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=[0.4913725490196078, 0.4823529411764706, 0.4466666666666667], std=[0.24705882352941178, 0.24352941176470588, 0.2615686274509804])\n",
      "                         )\n",
      "    Target Transforms (if any): None Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: .\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=[0.4913725490196078, 0.4823529411764706, 0.4466666666666667], std=[0.24705882352941178, 0.24352941176470588, 0.2615686274509804])\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# prepare the data set - here CIFAR\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "transform = transforms.Compose([transforms.ToTensor(),normalize,])\n",
    "trainset    = torchvision.datasets.CIFAR10(root='.', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "testset     = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "classes     = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(trainset,testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Train the Network & Get Converged Weights ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.215017Z",
     "start_time": "2019-05-12T00:10:34.989311Z"
    },
    "code_folding": [
     6,
     34
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 0\tTrain loss: 1.857\tTrain Acc: 30.87375\n",
      "\tEpoch: 0\tTest loss: 1.61\tTest Acc: 40.6893755\n",
      "==========================\n",
      "\tEpoch: 1\tTrain loss: 1.423\tTrain Acc: 48.36425\n",
      "\tEpoch: 1\tTest loss: 1.188\tTest Acc: 57.485625\n",
      "==========================\n",
      "\tEpoch: 2\tTrain loss: 1.125\tTrain Acc: 60.03625\n",
      "\tEpoch: 2\tTest loss: 1.04\tTest Acc: 63.2993755\n",
      "==========================\n",
      "\tEpoch: 3\tTrain loss: 0.966\tTrain Acc: 66.20855\n",
      "\tEpoch: 3\tTest loss: 0.933\tTest Acc: 67.453755\n",
      "==========================\n",
      "\tEpoch: 4\tTrain loss: 0.851\tTrain Acc: 70.43255\n",
      "\tEpoch: 4\tTest loss: 0.885\tTest Acc: 69.340625\n",
      "==========================\n",
      "\tEpoch: 5\tTrain loss: 0.77\tTrain Acc: 73.324625\n",
      "\tEpoch: 5\tTest loss: 0.828\tTest Acc: 71.513125\n",
      "==========================\n",
      "Epoch: 6 i : 82 and loss: 0.967 acc: 0.6953125\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2f566071f15f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" i : \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" and loss: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" acc: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\tEpoch: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\tTrain loss: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\tTrain Acc: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcorrect_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "num_epoch   = 100\n",
    "acurracy_list_train = []; loss_list_train     = []; acurracy_list_test  = []; loss_list_test     = []\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    correct_train    = 0; total_loss_train = 0; total_train      = 0 \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "    \n",
    "        # modify the inputs\n",
    "        batch_size  = inputs.size(0)\n",
    "        total_train = total_train + batch_size\n",
    "        inputs  = Variable(inputs);      targets = Variable(targets)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output and calculate loss and acc\n",
    "        outputs = net(inputs)\n",
    "        loss    = criterion(outputs, targets)\n",
    "        total_loss_train   = total_loss_train + loss.item()*batch_size\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_train      = correct_train + predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # update\n",
    "        loss.backward(); optimizer.step()\n",
    "        \n",
    "        # print\n",
    "        sys.stdout.write(\"Epoch: \"+str(epoch)+\" i : \"+str(batch_idx+1)+\" and loss: \"+str(np.around(loss.item(),3))+\" acc: \"+str(predicted.eq(targets).sum().item()/batch_size)+\"\\r\")\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\tEpoch: \"+str(epoch)+\"\\tTrain loss: \"+str(np.around(total_loss_train/total_train,3))+\"\\tTrain Acc: \"+str(100.*correct_train/total_train)+\"\\n\")\n",
    "    loss_list_train.append(total_loss_train/total_train) ;acurracy_list_train.append(100.*correct_train/total_train)\n",
    "    \n",
    "    correct = 0; total_loss = 0; total = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "            inputs  = Variable(inputs); targets = Variable(targets)\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            total_loss   = total_loss + loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct      = correct + predicted.eq(targets).sum().item()\n",
    "            # print\n",
    "            sys.stdout.write(\"Epoch: \"+str(epoch)+\" i : \"+str(batch_idx+1)+\" and loss: \"+str(np.around(loss.item(),3))+\" acc: \"+str(predicted.eq(targets).sum().item()/batch_size)+\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\tEpoch: \"+str(epoch)+\"\\tTest loss: \"+str(np.around(total_loss/total,3))+\"\\tTest Acc: \"+str(100.*correct/total)+\"\\n\")\n",
    "    loss_list_test.append(total_loss/total) ;acurracy_list_test.append(100.*correct/total)\n",
    "    sys.stdout.write(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.230974Z",
     "start_time": "2019-05-12T00:10:20.992Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the converged weights and plot them save the accuracy\n",
    "converged_weights = get_weights(net)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121); \n",
    "plt.plot(acurracy_list_train,label='Train Acc')\n",
    "plt.plot(acurracy_list_test,label='Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122); \n",
    "plt.plot(loss_list_train,label='Train Acc')\n",
    "plt.plot(loss_list_test,label='Test Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save('a_acurracy_list_train.npy', np.asarray(acurracy_list_train))\n",
    "np.save('a_acurracy_list_test.npy', np.asarray(acurracy_list_test))\n",
    "np.save('a_loss_list_train.npy', np.asarray(loss_list_train))\n",
    "np.save('a_loss_list_test.npy', np.asarray(loss_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.231971Z",
     "start_time": "2019-05-12T00:10:21.004Z"
    },
    "code_folding": [
     1,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# create help functions\n",
    "def viz_histogram_weights(converged_weights, direction1,direction2,title=\"None\"):\n",
    "    plt.figure(figsize=(55,55//9))\n",
    "    plt.suptitle(title, fontsize=20, y=1.15)\n",
    "    for layer_index in range(len(converged_weights)):\n",
    "        plt.subplot(1,9,layer_index+1)\n",
    "        plt.title(\"Layer : \" + str(layer_index))\n",
    "        plt.hist(converged_weights[layer_index].cpu().numpy().ravel(),50,alpha=0.6,label='Weight')\n",
    "        plt.hist(direction1[layer_index].cpu().numpy().ravel(),50,alpha=0.2,label='Direction 1')\n",
    "        plt.hist(direction2[layer_index].cpu().numpy().ravel(),50,alpha=0.2,label='Direction 2')\n",
    "        plt.yticks([])\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "def create_viz(loss_list,acc_list,title=\"none\"):\n",
    "    \n",
    "    # plot the loss functions\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Original Contour\")\n",
    "    CS = plt.contour(xcoord_mesh, ycoord_mesh, loss_list,  10, zorder=1, cmap='terrain', linestyles='--')\n",
    "    plt.clabel(CS, inline=1, fontsize=8)\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Original Contour with Color\")\n",
    "    plt.contour(xcoord_mesh, ycoord_mesh, loss_list,  10, zorder=1, cmap='terrain', linestyles='--')\n",
    "    CS = plt.contourf(xcoord_mesh, ycoord_mesh, loss_list, 10, zorder=1, cmap='terrain', linestyles='--')   \n",
    "    plt.clabel(CS, fontsize=12,inline=0,fmt = '%2.1f')\n",
    "    plt.colorbar(CS)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Log Scale\")\n",
    "    CS = plt.contour(xcoord_mesh, ycoord_mesh, np.log(loss_list+1e-8),10,zorder=1, cmap='terrain', linestyles='--'); \n",
    "    plt.clabel(CS, fontsize=8,inline=1)\n",
    "\n",
    "    plt.savefig(title)\n",
    "    plt.show()\n",
    "    \n",
    "    data = [\n",
    "        go.Surface(\n",
    "            x=xcoord_mesh,y=ycoord_mesh,\n",
    "            z=(loss_list.max()-loss_list.min())*(acc_list-acc_list.min())/(acc_list.max()-acc_list.min()+1e-8)+loss_list.min(),\n",
    "            showscale=False, opacity=0.6,colorscale='Cividis',\n",
    "        ),\n",
    "        go.Surface(\n",
    "            x=xcoord_mesh,y=ycoord_mesh,z=loss_list,colorscale='Jet',opacity=0.9,\n",
    "            contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,project=dict(z=True),),\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    layout = go.Layout(title='Loss / Accuracy',autosize=True,scene=dict(camera=dict(eye=dict(x=1.87, y=0.88, z=-0.64))),margin=dict(l=65,r=50,b=65,t=90))\n",
    "    fig    = go.Figure(data=data,layout=layout); iplot(fig); plt.show()\n",
    "\n",
    "    data = [\n",
    "        go.Surface(\n",
    "            x=xcoord_mesh,y=ycoord_mesh,\n",
    "            z=(np.log(loss_list).max()-np.log(loss_list).min())*(acc_list-acc_list.min())/(acc_list.max()-acc_list.min()+1e-8)+np.log(loss_list).min(),\n",
    "            showscale=False, opacity=0.6,colorscale='Cividis',\n",
    "        ),\n",
    "        go.Surface(\n",
    "            x=xcoord_mesh,y=ycoord_mesh,z=np.log(loss_list),colorscale='Jet',opacity=0.9,\n",
    "            contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,project=dict(z=True),),\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    layout = go.Layout(title='Log Scale Loss / Accuracy',autosize=True,scene=dict(camera=dict(eye=dict(x=1.87, y=0.88, z=-0.64))),margin=dict(l=65,r=50,b=65,t=90))\n",
    "    fig    = go.Figure(data=data,layout=layout); iplot(fig); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.233967Z",
     "start_time": "2019-05-12T00:10:21.008Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the coordinates\n",
    "numebr_of_points = 21 ; small_range = -1.0 ; large_range =  1.0\n",
    "\n",
    "xcoordinates = np.linspace(small_range, large_range, num=numebr_of_points) \n",
    "ycoordinates = np.linspace(small_range, large_range, num=numebr_of_points) \n",
    "\n",
    "xcoord_mesh, ycoord_mesh = np.meshgrid(xcoordinates, ycoordinates)\n",
    "inds = np.array(range(numebr_of_points**2))\n",
    "s1   = xcoord_mesh.ravel()[inds]\n",
    "s2   = ycoord_mesh.ravel()[inds]\n",
    "coordinate = np.c_[s1,s2]\n",
    "print('From ',small_range,' to ',large_range,' with ',numebr_of_points,' total number of coordinate: ', numebr_of_points**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.235961Z",
     "start_time": "2019-05-12T00:10:21.011Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'relu.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ==== Play Around with the Weigths ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.237955Z",
     "start_time": "2019-05-12T00:10:21.025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "copy_of_the_weights = [ w.clone() for w in converged_weights]\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.240948Z",
     "start_time": "2019-05-12T00:10:21.027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random_direction1 = get_random_weights(copy_of_the_weights)\n",
    "random_direction2 = [w.clone() for w in random_direction1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.242943Z",
     "start_time": "2019-05-12T00:10:21.030Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d,w in zip(random_direction1,copy_of_the_weights):\n",
    "    normalize_direction(d,w,'filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.244937Z",
     "start_time": "2019-05-12T00:10:21.034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for d,w in zip(random_direction2,copy_of_the_weights):\n",
    "    d_re   = d.view((d.shape[0],-1))\n",
    "    d_norm = d_re.norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "    \n",
    "    w_re   = w.view((w.shape[0],-1))\n",
    "    w_norm = w_re.norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "    temp.append(d.cuda() * (w_norm.cuda()/(d_norm.cuda()+1e-10)))\n",
    "    d.data      =  d.cuda() * (w_norm.cuda()/(d_norm.cuda()+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.246931Z",
     "start_time": "2019-05-12T00:10:21.037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for x, xx in zip(random_direction1,random_direction2):\n",
    "    print(np.allclose(x.cpu().numpy(),xx.cpu().numpy()))\n",
    "    \n",
    "for x, xx in zip(random_direction1,temp):\n",
    "    print(np.allclose(x.cpu().numpy(),xx.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.247928Z",
     "start_time": "2019-05-12T00:10:21.041Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "viz_histogram_weights(copy_of_the_weights,random_direction1,random_direction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ==== 1. Random - Filter Normalization ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.248925Z",
     "start_time": "2019-05-12T00:10:21.052Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random_direction1 = get_random_weights(copy_of_the_weights)\n",
    "random_direction2 = get_random_weights(copy_of_the_weights)\n",
    "\n",
    "for d1,d2,w in zip(random_direction1,random_direction2,copy_of_the_weights):\n",
    "    \n",
    "    w_norm  = w.view((w.shape[0],-1))  .norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "    d_norm1 = d1.view((d1.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "    d_norm2 = d2.view((d2.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "    \n",
    "    d1.data = d1.cuda() * (w_norm/(d_norm1.cuda()+1e-10))\n",
    "    d2.data = d2.cuda() * (w_norm/(d_norm2.cuda()+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.250921Z",
     "start_time": "2019-05-12T00:10:21.055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "viz_histogram_weights(copy_of_the_weights,random_direction1,random_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.252915Z",
     "start_time": "2019-05-12T00:10:21.058Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the evaluation\n",
    "loss_list = np.zeros((numebr_of_points,numebr_of_points)); acc_list  = np.zeros((numebr_of_points,numebr_of_points))\n",
    "col_value = 0\n",
    "\n",
    "for count, ind in enumerate(inds):\n",
    "    \n",
    "    # change the weight values\n",
    "    coord   = coordinate[count]\n",
    "    changes = [d0.cuda()*coord[0] + d1.cuda()*coord[1] for (d0, d1) in zip(random_direction1, random_direction2)]\n",
    "    for (p, w, d) in zip(net.parameters(), weight, changes): p.data = w + d\n",
    "\n",
    "    # start the evaluation\n",
    "    correct = 0; total_loss = 0; total = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total      = total + batch_size\n",
    "            \n",
    "            inputs, targets = Variable(inputs).cuda(),Variable(targets).cuda()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            total_loss   = total_loss + loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct      = correct + predicted.eq(targets).sum().item()\n",
    "            sys.stdout.write(\"Coord: \"+str(coord)+\"\\tAcc: \"+str(predicted.eq(targets).sum().item())+\"\\tLoss: \"+str(np.around(loss.item(),3))+\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if batch_idx==10: break\n",
    "            \n",
    "        if count % 2 == 0 : sys.stdout.write(\"count: \"+str(count)+\"\\tCoord: \"+str(coord)+\"\\t\\tAcc: \"+str(100.*correct/total)+\"\\tLoss: \"+str(np.around(total_loss/total,3))+\"\\n\")\n",
    "        \n",
    "    # store value \n",
    "    loss_list[col_value][ind%numebr_of_points] = total_loss/total\n",
    "    acc_list [col_value][ind%numebr_of_points] = 100.*correct/total\n",
    "    ind_compare = ind + 1\n",
    "    if ind_compare % numebr_of_points == 0 :  col_value = col_value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.254911Z",
     "start_time": "2019-05-12T00:10:21.060Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_viz(loss_list,acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ==== 2. Random - Orthogonal - whole norm ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.255909Z",
     "start_time": "2019-05-12T00:10:21.080Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.save('a_loss_list_1.npy', np.asarray(loss_list))\n",
    "# np.save('a_acc_list_1.npy', np.asarray(acc_list))\n",
    "random_direction1 = get_random_weights(copy_of_the_weights)\n",
    "random_direction2 = get_random_weights(copy_of_the_weights)\n",
    "\n",
    "for d1,d2,w in zip(random_direction1,random_direction2,copy_of_the_weights):\n",
    "    \n",
    "    if w.dim() == 1:\n",
    "        d1.data = torch.zeros_like(w)\n",
    "        d2.data = torch.zeros_like(w)\n",
    "        \n",
    "    elif w.shape[0] == 10:\n",
    "        d11,_ = tf.qr(d1.cpu().numpy())\n",
    "        d11   = d11.eval()\n",
    "        d22,_ = tf.qr(np.transpose(d2.cpu().numpy(),(2,3,1,0)))\n",
    "        d22   = np.transpose(d22.eval(),(3,2,0,1))\n",
    "        \n",
    "\n",
    "        d1.data   = torch.from_numpy(d11).cuda()\n",
    "        d2.data   = torch.from_numpy(d22).cuda()     \n",
    "        \n",
    "        w_norm  = w.view((w.shape[0],-1))  .norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm1 = d1.view((d1.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm2 = d2.view((d2.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "\n",
    "        d1.data = d1.cuda() * (w_norm/(d_norm1.cuda()+1e-10))\n",
    "        d2.data = d2.cuda() * (w_norm/(d_norm2.cuda()+1e-10))\n",
    "        \n",
    "    else:\n",
    "        d11,_ = tf.qr(d1.cpu().numpy())\n",
    "        d11   = d11.eval()\n",
    "        d22,_ = tf.qr(np.transpose(d2.cpu().numpy(),(2,3,0,1)))\n",
    "        d22   = np.transpose(d22.eval(),(2,3,0,1))\n",
    "        print(d11.shape,d22.shape)\n",
    "\n",
    "        d1.data   = torch.from_numpy(d11).cuda()\n",
    "        d2.data   = torch.from_numpy(d22).cuda()     \n",
    "        \n",
    "        w_norm  = w.view((w.shape[0],-1))  .norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm1 = d1.view((d1.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm2 = d2.view((d2.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "\n",
    "        d1.data = d1.cuda() * (w_norm/(d_norm1.cuda()+1e-10))\n",
    "        d2.data = d2.cuda() * (w_norm/(d_norm2.cuda()+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.257903Z",
     "start_time": "2019-05-12T00:10:21.082Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_histogram_weights(copy_of_the_weights,random_direction1,random_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.260895Z",
     "start_time": "2019-05-12T00:10:21.084Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the evaluation\n",
    "loss_list = np.zeros((numebr_of_points,numebr_of_points)); acc_list  = np.zeros((numebr_of_points,numebr_of_points))\n",
    "col_value = 0\n",
    "\n",
    "for count, ind in enumerate(inds):\n",
    "    \n",
    "    # change the weight values\n",
    "    coord   = coordinate[count]\n",
    "    changes = [d0.cuda()*coord[0] + d1.cuda()*coord[1] for (d0, d1) in zip(random_direction1, random_direction2)]\n",
    "    for (p, w, d) in zip(net.parameters(), weight, changes): p.data = w + d\n",
    "\n",
    "    # start the evaluation\n",
    "    correct = 0; total_loss = 0; total = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total      = total + batch_size\n",
    "            \n",
    "            inputs, targets = Variable(inputs).cuda(),Variable(targets).cuda()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            total_loss   = total_loss + loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct      = correct + predicted.eq(targets).sum().item()\n",
    "            sys.stdout.write(\"Coord: \"+str(coord)+\"\\tAcc: \"+str(predicted.eq(targets).sum().item())+\"\\tLoss: \"+str(np.around(loss.item(),3))+\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if batch_idx==10: break\n",
    "            \n",
    "        if count % 2 == 0 : sys.stdout.write(\"count: \"+str(count)+\"\\tCoord: \"+str(coord)+\"\\t\\tAcc: \"+str(100.*correct/total)+\"\\tLoss: \"+str(np.around(total_loss/total,3))+\"\\n\")\n",
    "        \n",
    "    # store value \n",
    "    loss_list[col_value][ind%numebr_of_points] = total_loss/total\n",
    "    acc_list [col_value][ind%numebr_of_points] = 100.*correct/total\n",
    "    ind_compare = ind + 1\n",
    "    if ind_compare % numebr_of_points == 0 :  col_value = col_value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.262889Z",
     "start_time": "2019-05-12T00:10:21.087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "create_viz(loss_list,acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== 3. Weight - Orthogonal - whole norm ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.264885Z",
     "start_time": "2019-05-12T00:10:21.099Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# np.save('a_loss_list_2.npy', np.asarray(loss_list))\n",
    "# np.save('a_acc_list_2.npy', np.asarray(acc_list))\n",
    "random_direction1 = []\n",
    "random_direction2 = []\n",
    "\n",
    "for w in copy_of_the_weights:\n",
    "    \n",
    "    if w.dim() == 1: \n",
    "        random_direction1.append(torch.zeros_like(w))\n",
    "        random_direction2.append(torch.zeros_like(w))\n",
    "        \n",
    "    else:\n",
    "        random_vector = w.clone().cpu().numpy()\n",
    "        \n",
    "        random_vector1 = random_vector - random_vector.mean((2,3),keepdims=True)\n",
    "        random_vector2 = random_vector - random_vector.mean((0,1),keepdims=True)\n",
    "        random_vector2 = np.transpose(random_vector2,(2,3,0,1))\n",
    "        \n",
    "        sigma1 = tf.matmul(tf.transpose(random_vector1,(0,1,3,2)),random_vector1) / random_vector1.shape[3]\n",
    "        sigma2 = tf.matmul(tf.transpose(random_vector2,(0,1,3,2)),random_vector2) / random_vector2.shape[3]\n",
    "\n",
    "        s1,u1,v1 = tf.linalg.svd(sigma1,False)\n",
    "        s2,u2,v2 = tf.linalg.svd(sigma2,False)\n",
    "        \n",
    "        tmp1 =  tf.matmul(u1,1/(tf.sqrt(tf.linalg.diag(s1))+1e-5))\n",
    "        tmp1 = tmp1 @ tf.transpose(u1,(0,1,3,2))\n",
    "        \n",
    "        tmp2 =  tf.matmul(u2,1/(tf.sqrt(tf.linalg.diag(s2))+1e-5))\n",
    "        tmp2 = tmp2 @ tf.transpose(u2,(0,1,3,2))\n",
    "        \n",
    "        random_vector1 = random_vector1 @ tf.transpose(tmp1,(0,1,3,2))\n",
    "        random_vector2 = random_vector2 @ tf.transpose(tmp2,(0,1,3,2))\n",
    "        random_vector2 = tf.transpose(random_vector2,(2,3,0,1))\n",
    "        \n",
    "        random_vector1 = torch.from_numpy(random_vector1.eval()).cuda()\n",
    "        random_vector2 = torch.from_numpy(random_vector2.eval()).cuda()\n",
    "\n",
    "        w_norm  = w.view((w.shape[0],-1))  .norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm1 = random_vector1.view((random_vector1.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm2 = random_vector2.view((random_vector2.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "\n",
    "        random_vector1 = random_vector1 * (w_norm/(d_norm1.cuda()+1e-10))\n",
    "        random_vector2 = random_vector2 * (w_norm/(d_norm2.cuda()+1e-10))  \n",
    "        \n",
    "        print(random_vector1.shape)\n",
    "        print(random_vector2.shape)\n",
    "\n",
    "        random_direction1.append(random_vector1)\n",
    "        random_direction2.append(random_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.265881Z",
     "start_time": "2019-05-12T00:10:21.101Z"
    }
   },
   "outputs": [],
   "source": [
    "viz_histogram_weights(copy_of_the_weights,random_direction1,random_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.267876Z",
     "start_time": "2019-05-12T00:10:21.104Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start the evaluation\n",
    "loss_list = np.zeros((numebr_of_points,numebr_of_points)); acc_list  = np.zeros((numebr_of_points,numebr_of_points))\n",
    "col_value = 0\n",
    "\n",
    "for count, ind in enumerate(inds):\n",
    "    \n",
    "    # change the weight values\n",
    "    coord   = coordinate[count]\n",
    "    changes = [d0.cuda()*coord[0] + d1.cuda()*coord[1] for (d0, d1) in zip(random_direction1, random_direction2)]\n",
    "    for (p, w, d) in zip(net.parameters(), weight, changes): p.data = w + d\n",
    "\n",
    "    # start the evaluation\n",
    "    correct = 0; total_loss = 0; total = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total      = total + batch_size\n",
    "            \n",
    "            inputs, targets = Variable(inputs).cuda(),Variable(targets).cuda()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            total_loss   = total_loss + loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct      = correct + predicted.eq(targets).sum().item()\n",
    "            sys.stdout.write(\"Coord: \"+str(coord)+\"\\tAcc: \"+str(predicted.eq(targets).sum().item())+\"\\tLoss: \"+str(np.around(loss.item(),3))+\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if batch_idx==10: break\n",
    "            \n",
    "        if count % 2 == 0 : sys.stdout.write(\"count: \"+str(count)+\"\\tCoord: \"+str(coord)+\"\\t\\tAcc: \"+str(100.*correct/total)+\"\\tLoss: \"+str(np.around(total_loss/total,3))+\"\\n\")\n",
    "        \n",
    "    # store value \n",
    "    loss_list[col_value][ind%numebr_of_points] = total_loss/total\n",
    "    acc_list [col_value][ind%numebr_of_points] = 100.*correct/total\n",
    "    ind_compare = ind + 1\n",
    "    if ind_compare % numebr_of_points == 0 :  col_value = col_value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.269871Z",
     "start_time": "2019-05-12T00:10:21.108Z"
    }
   },
   "outputs": [],
   "source": [
    "create_viz(loss_list,acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== 4. Weight - PCA - whole norm ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.270868Z",
     "start_time": "2019-05-12T00:10:21.121Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# np.save('a_loss_list_3.npy', np.asarray(loss_list))\n",
    "# np.save('a_acc_list_3.npy', np.asarray(acc_list))\n",
    "random_direction1 = []\n",
    "random_direction2 = []\n",
    "\n",
    "for w in copy_of_the_weights:\n",
    "    \n",
    "    if w.dim() == 1: \n",
    "        random_direction1.append(torch.zeros_like(w))\n",
    "        random_direction2.append(torch.zeros_like(w))\n",
    "        \n",
    "    else:\n",
    "        random_vector = w.clone().cpu().numpy()\n",
    "        \n",
    "        random_vector1 = random_vector - random_vector.mean((2,3),keepdims=True)\n",
    "        random_vector2 = random_vector - random_vector.mean((0,1),keepdims=True)\n",
    "        random_vector2 = np.transpose(random_vector2,(2,3,0,1))\n",
    "        \n",
    "        s1,u1,v1 = tf.linalg.svd(random_vector1,False)\n",
    "        s2,u2,v2 = tf.linalg.svd(random_vector2,False)\n",
    "        \n",
    "        random_vector1 = u1 @ tf.linalg.diag(s1)[:,:,:,:1] @ tf.transpose(v1,(0,1,3,2))[:,:,:1,:]\n",
    "        random_vector2 = u2 @ tf.linalg.diag(s2)[:,:,:,:1] @ tf.transpose(v2,(0,1,3,2))[:,:,:1,:]\n",
    "        random_vector2 = tf.transpose(random_vector2,(2,3,0,1))\n",
    "        \n",
    "        random_vector1 = torch.from_numpy(random_vector1.eval()).cuda()\n",
    "        random_vector2 = torch.from_numpy(random_vector2.eval()).cuda()\n",
    "\n",
    "        w_norm  = w.view((w.shape[0],-1))  .norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm1 = random_vector1.view((random_vector1.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "        d_norm2 = random_vector2.view((random_vector2.shape[0],-1)).norm(dim=(1),keepdim=True)[:,:,None,None]\n",
    "\n",
    "        random_vector1 = random_vector1 * (w_norm/(d_norm1.cuda()+1e-10))\n",
    "        random_vector2 = random_vector2 * (w_norm/(d_norm2.cuda()+1e-10))  \n",
    "        \n",
    "        print(random_vector1.shape)\n",
    "        print(random_vector2.shape)\n",
    "\n",
    "        random_direction1.append(random_vector1)\n",
    "        random_direction2.append(random_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.273860Z",
     "start_time": "2019-05-12T00:10:21.125Z"
    }
   },
   "outputs": [],
   "source": [
    "viz_histogram_weights(copy_of_the_weights,random_direction1,random_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.277850Z",
     "start_time": "2019-05-12T00:10:21.128Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start the evaluation\n",
    "loss_list = np.zeros((numebr_of_points,numebr_of_points)); acc_list  = np.zeros((numebr_of_points,numebr_of_points))\n",
    "col_value = 0\n",
    "\n",
    "for count, ind in enumerate(inds):\n",
    "    \n",
    "    # change the weight values\n",
    "    coord   = coordinate[count]\n",
    "    changes = [d0.cuda()*coord[0] + d1.cuda()*coord[1] for (d0, d1) in zip(random_direction1, random_direction2)]\n",
    "    for (p, w, d) in zip(net.parameters(), weight, changes): p.data = w + d\n",
    "\n",
    "    # start the evaluation\n",
    "    correct = 0; total_loss = 0; total = 0 \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total      = total + batch_size\n",
    "            \n",
    "            inputs, targets = Variable(inputs).cuda(),Variable(targets).cuda()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            total_loss   = total_loss + loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct      = correct + predicted.eq(targets).sum().item()\n",
    "            sys.stdout.write(\"Coord: \"+str(coord)+\"\\tAcc: \"+str(predicted.eq(targets).sum().item())+\"\\tLoss: \"+str(np.around(loss.item(),3))+\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if batch_idx==10: break\n",
    "            \n",
    "        if count % 2 == 0 : sys.stdout.write(\"count: \"+str(count)+\"\\tCoord: \"+str(coord)+\"\\t\\tAcc: \"+str(100.*correct/total)+\"\\tLoss: \"+str(np.around(total_loss/total,3))+\"\\n\")\n",
    "        \n",
    "    # store value \n",
    "    loss_list[col_value][ind%numebr_of_points] = total_loss/total\n",
    "    acc_list [col_value][ind%numebr_of_points] = 100.*correct/total\n",
    "    ind_compare = ind + 1\n",
    "    if ind_compare % numebr_of_points == 0 :  col_value = col_value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.279843Z",
     "start_time": "2019-05-12T00:10:21.131Z"
    }
   },
   "outputs": [],
   "source": [
    "create_viz(loss_list,acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T00:13:28.281838Z",
     "start_time": "2019-05-12T00:10:21.133Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('a_loss_list_4.npy', np.asarray(loss_list))\n",
    "np.save('a_acc_list_4.npy', np.asarray(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
