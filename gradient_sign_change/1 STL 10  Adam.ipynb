{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T00:21:34.034604Z",
     "start_time": "2019-06-18T00:21:22.247940Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "from keras.utils import to_categorical\n",
    "plt.style.use('seaborn-white'); os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "np.random.seed(678); tf.set_random_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T00:21:34.063540Z",
     "start_time": "2019-06-18T00:21:34.053387Z"
    },
    "code_folding": [
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# prepare STL 10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T01:26:57.138575Z",
     "start_time": "2019-06-18T01:26:57.094680Z"
    },
    "code_folding": [
     47,
     87
    ]
   },
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32) +  tf.exp(tf.cast(tf.less_equal(x,0),tf.float32) * x)\n",
    "def tf_iden(x):    return x\n",
    "def d_tf_iden(x):  return tf.ones_like(x)\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "class CNN():\n",
    "    \n",
    "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.padding    = 'SAME'\n",
    "        self.stride     = 1\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.padding= padding; self.stride = stride\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,self.stride,self.stride,1],padding=self.padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,iter_num):\n",
    "        grad_part_1 = gradient \n",
    "        grad_part_2 = self.d_act(self.layer) \n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "\n",
    "        grad     = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
    "            strides=[1,self.stride,self.stride,1],padding=self.padding\n",
    "        )/batch_size\n",
    "\n",
    "        grad_pass = tf.nn.conv2d_backprop_input(input_sizes = self.input.shape,filter= self.w,  out_backprop = grad_middle,\n",
    "            strides=[1,self.stride,self.stride,1],padding=self.padding\n",
    "        )\n",
    "\n",
    "        \n",
    "        grad_pos      = tf.where(grad>0,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_pos_mean = tf.reduce_sum(grad_pos*grad)/tf.reduce_sum(grad_pos)\n",
    "        \n",
    "        grad_neg      = tf.where(grad<0,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_neg_mean = tf.reduce_sum(grad_neg*grad)/tf.reduce_sum(grad_neg)\n",
    "        \n",
    "        grad_between    = tf.where(grad_neg_mean<grad,tf.ones_like(grad),tf.zeros_like(grad)) * tf.where(grad_pos_mean>grad,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_sign_chage = grad_between     * grad * -2\n",
    "        grad_sign_keep  = (1-grad_between) * grad *  1\n",
    "        \n",
    "        grad_new = grad_sign_chage + grad_sign_keep\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_new)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_new ** 2)   ))\n",
    "        m_hat = self.m / (1-tf.pow(beta1,iter_num))\n",
    "        v_hat = self.v / (1-tf.pow(beta2,iter_num))\n",
    "        update_w.append(tf.assign( self.w,self.w - learning_rate*( m_hat/(tf.sqrt(v_hat) + adam_e)   )))\n",
    "        return grad_pass,update_w  \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis=None):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = (0,1,2)\n",
    "        \n",
    "    def feedforward(self,input,training_phase,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = (1,2,3)\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:57.321Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "x_train,y_train = read_all_images('../Dataset/STL10/stl10_binary/train_x.bin'),read_labels('../Dataset/STL10/stl10_binary/train_y.bin')\n",
    "x_test,y_test   = read_all_images('../Dataset/STL10/stl10_binary/test_x.bin'),read_labels('../Dataset/STL10/stl10_binary/test_y.bin')\n",
    "\n",
    "y_train = to_categorical(y_train-1)\n",
    "y_test  = to_categorical(y_test-1)\n",
    "\n",
    "x_train = (x_train-x_train.min((1,2),keepdims=True))/(x_train.max((1,2),keepdims=True)-x_train.min((1,2),keepdims=True))\n",
    "x_train = (x_train-x_train.mean((1,2),keepdims=True))/(x_train.std((1,2),keepdims=True))\n",
    "x_test = (x_test-x_test.min((1,2),keepdims=True))/(x_test.max((1,2),keepdims=True)-x_test.min((1,2),keepdims=True))\n",
    "x_test = (x_test-x_test.mean((1,2),keepdims=True))/(x_test.std((1,2),keepdims=True))\n",
    "\n",
    "print(x_train.shape,y_train.shape,x_train.min(),x_train.max())\n",
    "print(x_test.shape,y_test.shape,x_test.min(),x_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:57.552Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare hyper parameter\n",
    "learning_rate = 0.0008 ; beta1 = 0.9; beta2 = 0.999; adam_e = 1e-8\n",
    "num_epoch     = 30    ; batch_size = 50 ; print_size = 1; \n",
    "channel_sizes = 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:57.863Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare layer\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "avg_lss_train = 0; avg_lss_test  = 0; train_lss = [];test_lss = []\n",
    "\n",
    "l1   = CNN(3,3,channel_sizes)\n",
    "l2   = CNN(3,channel_sizes,channel_sizes)\n",
    "l3   = CNN(3,channel_sizes,channel_sizes)\n",
    "\n",
    "l4   = CNN(3,channel_sizes,channel_sizes)\n",
    "l5   = CNN(3,channel_sizes,channel_sizes)\n",
    "l6   = CNN(3,channel_sizes,channel_sizes)\n",
    "\n",
    "l7   = CNN(3,channel_sizes,channel_sizes)\n",
    "l8   = CNN(1,channel_sizes,channel_sizes)\n",
    "l9   = CNN(1,channel_sizes,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:58.238Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph\n",
    "x = tf.placeholder(shape=[batch_size,96,96,3],dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[batch_size,10],     dtype=tf.float32)\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "iter_num = tf.placeholder_with_default(1.0,())\n",
    "\n",
    "layer1  = l1. feedforward(x,padding='VALID',stride=2)    \n",
    "layer2  = l2.feedforward(layer1,padding='VALID') \n",
    "layer3  = l3.feedforward(layer2,padding='VALID',stride=2); \n",
    "\n",
    "layer4  = l4.feedforward(layer3,padding='VALID');      \n",
    "layer5  = l5.feedforward(layer4,padding='VALID');      \n",
    "layer6  = l6.feedforward(layer5,padding='VALID',stride=2)\n",
    "\n",
    "layer7  = l7.feedforward(layer6,padding='VALID')\n",
    "layer8  = l8.feedforward(layer7)\n",
    "layer9  = l9.feedforward(layer8)\n",
    "\n",
    "final_global = tf.reduce_mean(layer9,[1,2])\n",
    "final_soft   = tf_softmax(final_global) ; \n",
    "cost         = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_global,labels=y))\n",
    "accuracy     = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(final_soft, 1), tf.argmax(y, 1)),tf.float32) )\n",
    "\n",
    "gradient     = tf.tile((final_soft-y)[:,None,None,:],(1,6,6,1))\n",
    "\n",
    "grad9, grad9_up  = l9. backprop(gradient,iter_num)\n",
    "grad8, grad8_up  = l8. backprop(grad9,iter_num)\n",
    "grad7, grad7_up  = l7. backprop(grad8,iter_num)\n",
    "\n",
    "grad6, grad6_up  = l6. backprop(grad7,iter_num)\n",
    "grad5, grad5_up  = l5. backprop(grad6,iter_num)\n",
    "grad4, grad4_up  = l4. backprop(grad5,iter_num)\n",
    "\n",
    "grad3,grad3_up   = l3. backprop(grad4,iter_num)\n",
    "grad2,grad2_up   = l2. backprop(grad3,iter_num)\n",
    "grad1,grad1_up   = l1. backprop(grad2,iter_num)\n",
    "\n",
    "grad_update =  grad9_up + grad8_up + grad7_up +\\\n",
    "               grad6_up + grad5_up + grad4_up +\\\n",
    "               grad3_up + grad2_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:58.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# start the session\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T01:26:59.089Z"
    }
   },
   "outputs": [],
   "source": [
    "# start the training \n",
    "for iter in range(1,num_epoch+1):\n",
    "    for current_batch_index in range(0,len(x_train),batch_size):\n",
    "        current_data  = x_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost,grad_update],feed_dict={x:current_data,y:current_label,iter_num:iter})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_train)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        avg_lss_train = avg_lss_train + sess_results[1]\n",
    "    for current_batch_index in range(0,len(x_test), batch_size):\n",
    "        current_data  = x_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_test)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_test = avg_acc_test + sess_results[0]\n",
    "        avg_lss_test = avg_lss_test + sess_results[1]\n",
    "    # ======================== print reset ========================  \n",
    "    print(\"Current: \"+ str(iter) + \n",
    "          \"\\tTrain Acc: \"  + str(avg_acc_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTrain Cost: \" + str(avg_lss_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTest Acc: \"   + str(avg_acc_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tTest Cost: \"  + str(avg_lss_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tLR: \" + str(learning_rate) )\n",
    "    avg_acc_train   = 0 ; avg_acc_test  = 0 ; avg_lss_train = 0 ; avg_lss_test  = 0\n",
    "    x_train,y_train = shuffle(x_train,y_train); x_test,y_test   = shuffle(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T00:12:01.633084Z",
     "start_time": "2019-06-05T00:12:01.531955Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T01:17:45.508045Z",
     "start_time": "2019-06-18T01:17:45.410305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9791961  1.3355689 -0.5717732  1.6240884  0.5671745]\n",
      "[0. 1. 0. 1. 1.]\n",
      "1.1756105\n",
      "[1. 0. 1. 0. 0.]\n",
      "-0.7754846\n",
      "[-0.         0.        -0.5717732  0.         0.5671745]\n",
      "[-0.9791961  1.3355689 -0.         1.6240884  0.       ]\n"
     ]
    }
   ],
   "source": [
    "# see \n",
    "temp = tf.random_normal([5],stddev=1,seed=4)\n",
    "\n",
    "temp_pos      = tf.where(temp>0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_pos_mean = tf.reduce_sum(temp_pos*temp)/tf.reduce_sum(temp_pos)\n",
    "\n",
    "temp_neg      = tf.where(temp<0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_neg_mean = tf.reduce_sum(temp_neg*temp)/tf.reduce_sum(temp_neg)\n",
    "\n",
    "temp_between  = tf.where(temp_neg_mean<temp,tf.ones_like(temp),tf.zeros_like(temp)) * tf.where(temp_pos_mean>temp,tf.ones_like(temp),tf.zeros_like(temp)) \n",
    "\n",
    "temp1 = temp_between * temp\n",
    "temp2 = (1-temp_between) * temp\n",
    "\n",
    "all_of_data = sess.run([temp,\n",
    "                        temp_pos,temp_pos_mean,\n",
    "                        temp_neg,temp_neg_mean,temp1,temp2])\n",
    "\n",
    "print(all_of_data[0])\n",
    "\n",
    "print(all_of_data[1])\n",
    "print(all_of_data[2])\n",
    "print(all_of_data[3])\n",
    "\n",
    "print(all_of_data[4])\n",
    "print(all_of_data[5])\n",
    "print(all_of_data[6])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
