{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T00:21:34.034604Z",
     "start_time": "2019-06-18T00:21:22.247940Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "from keras.utils import to_categorical\n",
    "plt.style.use('seaborn-white'); os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "np.random.seed(678); tf.set_random_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T00:21:34.063540Z",
     "start_time": "2019-06-18T00:21:34.053387Z"
    },
    "code_folding": [
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# prepare STL 10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T03:19:17.908886Z",
     "start_time": "2019-06-18T03:19:14.538901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) (5000, 10) 0.0 1.0\n",
      "(8000, 96, 96, 3) (8000, 10) 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "x_train,y_train = read_all_images('../Dataset/STL10/stl10_binary/train_x.bin'),read_labels('../Dataset/STL10/stl10_binary/train_y.bin')\n",
    "x_test,y_test   = read_all_images('../Dataset/STL10/stl10_binary/test_x.bin'),read_labels('../Dataset/STL10/stl10_binary/test_y.bin')\n",
    "\n",
    "y_train = to_categorical(y_train-1)\n",
    "y_test  = to_categorical(y_test-1)\n",
    "\n",
    "x_train = (x_train-x_train.min((1,2),keepdims=True))/(x_train.max((1,2),keepdims=True)-x_train.min((1,2),keepdims=True))\n",
    "# x_train = (x_train-x_train.mean((1,2),keepdims=True))/(x_train.std((1,2),keepdims=True))\n",
    "x_test = (x_test-x_test.min((1,2),keepdims=True))/(x_test.max((1,2),keepdims=True)-x_test.min((1,2),keepdims=True))\n",
    "# x_test = (x_test-x_test.mean((1,2),keepdims=True))/(x_test.std((1,2),keepdims=True))\n",
    "\n",
    "print(x_train.shape,y_train.shape,x_train.min(),x_train.max())\n",
    "print(x_test.shape,y_test.shape,x_test.min(),x_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T04:19:14.657228Z",
     "start_time": "2019-06-18T04:19:14.654221Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare hyper parameter\n",
    "learning_rate = 0.0001 ; beta1 = 0.9; beta2 = 0.999; adam_e = 1e-8\n",
    "num_epoch     = 100    ; batch_size = 50 ; print_size = 1; \n",
    "channel_sizes = 192 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T04:19:15.022277Z",
     "start_time": "2019-06-18T04:19:14.975410Z"
    },
    "code_folding": [
     60,
     100
    ]
   },
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32) +  tf.exp(tf.cast(tf.less_equal(x,0),tf.float32) * x)\n",
    "def tf_iden(x):    return x\n",
    "def d_tf_iden(x):  return tf.ones_like(x)\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "class CNN():\n",
    "    \n",
    "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.padding    = 'SAME'\n",
    "        self.stride     = 1\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05,seed=567))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.padding= padding; self.stride = stride\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,self.stride,self.stride,1],padding=self.padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,iter_num):\n",
    "        grad_part_1 = gradient \n",
    "        grad_part_2 = self.d_act(self.layer) \n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "\n",
    "        grad     = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
    "            strides=[1,self.stride,self.stride,1],padding=self.padding\n",
    "        )/batch_size\n",
    "\n",
    "        grad_pass = tf.nn.conv2d_backprop_input(input_sizes = self.input.shape,filter= self.w,  out_backprop = grad_middle,\n",
    "            strides=[1,self.stride,self.stride,1],padding=self.padding\n",
    "        )\n",
    "\n",
    "        dimension_shape = (0,1)\n",
    "        grad_pos      = tf.where(grad>0,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_pos_mean = tf.reduce_sum(grad_pos*grad,dimension_shape,keepdims=True)/tf.reduce_sum(grad_pos,dimension_shape,keepdims=True)\n",
    "        \n",
    "        grad_neg      = tf.where(grad<0,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_neg_mean = tf.reduce_sum(grad_neg*grad,dimension_shape,keepdims=True)/tf.reduce_sum(grad_neg,dimension_shape,keepdims=True)\n",
    "        \n",
    "        grad_between    = tf.where(grad_neg_mean<grad,tf.ones_like(grad),tf.zeros_like(grad)) * tf.where(grad_pos_mean>grad,tf.ones_like(grad),tf.zeros_like(grad))\n",
    "        grad_sign_chage = grad_between     * grad *  0\n",
    "        grad_sign_keep  = (1-grad_between) * grad *  1\n",
    "        \n",
    "        grad_new = grad_sign_chage + grad_sign_keep\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_new)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_new ** 2)   ))\n",
    "        m_hat = self.m / (1-tf.pow(beta1,iter_num))\n",
    "        v_hat = self.v / (1-tf.pow(beta2,iter_num))\n",
    "        update_w.append(tf.assign( self.w,self.w - learning_rate*( m_hat/(tf.sqrt(v_hat) + adam_e)   )))\n",
    "        return grad_pass,update_w  \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis=None):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = (0,1,2)\n",
    "        \n",
    "    def feedforward(self,input,training_phase,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = (1,2,3)\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T04:19:15.533619Z",
     "start_time": "2019-06-18T04:19:15.354696Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare layer\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "avg_lss_train = 0; avg_lss_test  = 0; train_lss = [];test_lss = []\n",
    "\n",
    "l1   = CNN(3,3,channel_sizes)\n",
    "l2   = CNN(3,channel_sizes,channel_sizes)\n",
    "l3   = CNN(3,channel_sizes,channel_sizes)\n",
    "\n",
    "l4   = CNN(3,channel_sizes,channel_sizes)\n",
    "l5   = CNN(3,channel_sizes,channel_sizes)\n",
    "l6   = CNN(3,channel_sizes,channel_sizes)\n",
    "\n",
    "l7   = CNN(3,channel_sizes,channel_sizes)\n",
    "l8   = CNN(1,channel_sizes,channel_sizes)\n",
    "l9   = CNN(1,channel_sizes,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T04:19:16.493047Z",
     "start_time": "2019-06-18T04:19:15.794673Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph\n",
    "x = tf.placeholder(shape=[batch_size,96,96,3],dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[batch_size,10],     dtype=tf.float32)\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "iter_num = tf.placeholder_with_default(1.0,())\n",
    "\n",
    "layer1  = l1. feedforward(x,stride=2)    \n",
    "layer2  = l2.feedforward(layer1,stride=2) \n",
    "layer3  = l3.feedforward(layer2); \n",
    "\n",
    "layer4  = l4.feedforward(layer3,stride=2);      \n",
    "layer5  = l5.feedforward(layer4);      \n",
    "layer6  = l6.feedforward(layer5,stride=1)\n",
    "\n",
    "layer7  = l7.feedforward(layer6,stride=1)\n",
    "layer8  = l8.feedforward(layer7)\n",
    "layer9  = l9.feedforward(layer8)\n",
    "\n",
    "final_global = tf.reduce_mean(layer9,[1,2])\n",
    "final_soft   = tf_softmax(final_global) ; \n",
    "cost         = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_global,labels=y))\n",
    "accuracy     = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(final_soft, 1), tf.argmax(y, 1)),tf.float32) )\n",
    "\n",
    "gradient     = tf.tile((final_soft-y)[:,None,None,:],(1,12,12,1))\n",
    "\n",
    "grad9, grad9_up  = l9. backprop(gradient,iter_num)\n",
    "grad8, grad8_up  = l8. backprop(grad9,iter_num)\n",
    "grad7, grad7_up  = l7. backprop(grad8,iter_num)\n",
    "\n",
    "grad6, grad6_up  = l6. backprop(grad7,iter_num)\n",
    "grad5, grad5_up  = l5. backprop(grad6,iter_num)\n",
    "grad4, grad4_up  = l4. backprop(grad5,iter_num)\n",
    "\n",
    "grad3,grad3_up   = l3. backprop(grad4,iter_num)\n",
    "grad2,grad2_up   = l2. backprop(grad3,iter_num)\n",
    "grad1,grad1_up   = l1. backprop(grad2,iter_num)\n",
    "\n",
    "grad_update =  grad9_up + grad8_up + grad7_up +\\\n",
    "               grad6_up + grad5_up + grad4_up +\\\n",
    "               grad3_up + grad2_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T04:19:16.171Z"
    }
   },
   "outputs": [],
   "source": [
    "# start the session\n",
    "# sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T04:19:16.686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 1\tTrain Acc: 0.14980000037699936\tTrain Cost: 2.2671061325073243\tTest Acc: 0.15950000016018748\tTest Cost: 2.2216994643211363\tLR: 0.0001\n",
      "Current: 2\tTrain Acc: 0.1607999998703599\tTrain Cost: 2.177944750785828\tTest Acc: 0.16662499997764826\tTest Cost: 2.1581659995019438\tLR: 0.0001\n",
      "Current: 3\tTrain Acc: 0.19500000063329936\tTrain Cost: 2.1385667526721956\tTest Acc: 0.2137499992037192\tTest Cost: 2.131424367427826\tLR: 0.0001\n",
      "Current: 4\tTrain Acc: 0.2348000006377697\tTrain Cost: 2.0733310866355894\tTest Acc: 0.24512500006239862\tTest Cost: 2.0505366303026675\tLR: 0.0001\n",
      "Current: 5\tTrain Acc: 0.2516000007092953\tTrain Cost: 2.011458321809769\tTest Acc: 0.2586250002961606\tTest Cost: 1.9819840013980865\tLR: 0.0001\n",
      "Current: 6\tTrain Acc: 0.28559999987483026\tTrain Cost: 1.9006473350524902\tTest Acc: 0.28025000020861623\tTest Cost: 1.8806950621306897\tLR: 0.0001\n",
      "Current: 7\tTrain Acc: 0.3094000007212162\tTrain Cost: 1.809386395215988\tTest Acc: 0.3372500013560057\tTest Cost: 1.7689109817147255\tLR: 0.0001\n",
      "Current: 8\tTrain Acc: 0.3420000013709068\tTrain Cost: 1.7427554404735566\tTest Acc: 0.3550000010989606\tTest Cost: 1.717304202914238\tLR: 0.0001\n",
      "Current: 9\tTrain Acc: 0.3464000017940998\tTrain Cost: 1.7113901555538178\tTest Acc: 0.35025000004097817\tTest Cost: 1.74610583037138\tLR: 0.0001\n",
      "Current: 10\tTrain Acc: 0.3796000024676323\tTrain Cost: 1.6668640387058258\tTest Acc: 0.3612500010058284\tTest Cost: 1.7078340172767639\tLR: 0.0001\n",
      "Current: 11\tTrain Acc: 0.3749999997019768\tTrain Cost: 1.6635570275783538\tTest Acc: 0.3821250006556511\tTest Cost: 1.6657064110040665\tLR: 0.0001\n",
      "Current: 12\tTrain Acc: 0.39339999973773954\tTrain Cost: 1.6359928667545318\tTest Acc: 0.38062499975785613\tTest Cost: 1.6402009822428227\tLR: 0.0001\n",
      "Current: 13\tTrain Acc: 0.3989999994635582\tTrain Cost: 1.607784824371338\tTest Acc: 0.38737500067800285\tTest Cost: 1.627739617228508\tLR: 0.0001\n",
      "Current: 14\tTrain Acc: 0.4101999995112419\tTrain Cost: 1.578707344532013\tTest Acc: 0.40400000140070913\tTest Cost: 1.614892452210188\tLR: 0.0001\n",
      "Current: 15\tTrain Acc: 0.4151999986171722\tTrain Cost: 1.5642831599712372\tTest Acc: 0.39624999919906256\tTest Cost: 1.6132483087480067\tLR: 0.0001\n",
      "Current: 16\tTrain Acc: 0.4238000002503395\tTrain Cost: 1.5479553997516633\tTest Acc: 0.40137499989941716\tTest Cost: 1.5847264766693114\tLR: 0.0001\n",
      "Current: 17\tTrain Acc: 0.42320000037550926\tTrain Cost: 1.537128359079361\tTest Acc: 0.4127499995753169\tTest Cost: 1.5673319779336452\tLR: 0.0001\n",
      "Current: 18\tTrain Acc: 0.44699999898672105\tTrain Cost: 1.5089677083492279\tTest Acc: 0.42587499925866723\tTest Cost: 1.5508912719786168\tLR: 0.0001\n",
      "Current: 19\tTrain Acc: 0.44939999967813493\tTrain Cost: 1.5011242842674255\tTest Acc: 0.40800000010058285\tTest Cost: 1.585711169987917\tLR: 0.0001\n",
      "Current: 20\tTrain Acc: 0.4506000003218651\tTrain Cost: 1.4805760502815246\tTest Acc: 0.42962499922141434\tTest Cost: 1.5289542153477669\tLR: 0.0001\n",
      "Current: 21\tTrain Acc: 0.45959999918937683\tTrain Cost: 1.462146772146225\tTest Acc: 0.41887499997392297\tTest Cost: 1.5870750464498997\tLR: 0.0001\n",
      "Current: 22\tTrain Acc: 0.45919999793171884\tTrain Cost: 1.4572105157375335\tTest Acc: 0.43512499872595073\tTest Cost: 1.5158616200089454\tLR: 0.0001\n",
      "Current: 23\tTrain Acc: 0.4571999999880791\tTrain Cost: 1.4554082429409028\tTest Acc: 0.42812499925494196\tTest Cost: 1.542810932546854\tLR: 0.0001\n",
      "Current: 24\tTrain Acc: 0.4735999989509583\tTrain Cost: 1.4294248700141907\tTest Acc: 0.4427499993704259\tTest Cost: 1.5001741997897624\tLR: 0.0001\n",
      "Current: 25\tTrain Acc: 0.4827999994158745\tTrain Cost: 1.418192788362503\tTest Acc: 0.45312499850988386\tTest Cost: 1.488354842364788\tLR: 0.0001\n",
      "Current: 26\tTrain Acc: 0.483399997651577\tTrain Cost: 1.4004606676101685\tTest Acc: 0.44299999829381703\tTest Cost: 1.4911353275179864\tLR: 0.0001\n",
      "Current: 27\tTrain Acc: 0.48900000005960464\tTrain Cost: 1.3924989914894104\tTest Acc: 0.4508749986067414\tTest Cost: 1.4858417086303235\tLR: 0.0001\n",
      "Current: 28\tTrain Acc: 0.4899999979138374\tTrain Cost: 1.3896355998516083\tTest Acc: 0.455499997548759\tTest Cost: 1.4865557469427586\tLR: 0.0001\n",
      "Current: 29\tTrain Acc: 0.49639999866485596\tTrain Cost: 1.3712347280979156\tTest Acc: 0.45099999886006115\tTest Cost: 1.4668703757226467\tLR: 0.0001\n",
      "Current: 30\tTrain Acc: 0.4993999972939491\tTrain Cost: 1.3728401505947112\tTest Acc: 0.45824999939650296\tTest Cost: 1.4595622837543487\tLR: 0.0001\n",
      "Current: 31\tTrain Acc: 0.5086000019311905\tTrain Cost: 1.3463183975219726\tTest Acc: 0.46474999766796826\tTest Cost: 1.439987939596176\tLR: 0.0001\n",
      "Current: 32\tTrain Acc: 0.5048000007867813\tTrain Cost: 1.3576997315883637\tTest Acc: 0.4462499989196658\tTest Cost: 1.5002816051244736\tLR: 0.0001\n",
      "Current: 33\tTrain Acc: 0.5161999994516373\tTrain Cost: 1.3336550319194793\tTest Acc: 0.46724999938160183\tTest Cost: 1.4546385817229748\tLR: 0.0001\n",
      "Current: 34\tTrain Acc: 0.5121999990940094\tTrain Cost: 1.330236623287201\tTest Acc: 0.4546250001527369\tTest Cost: 1.483550590276718\tLR: 0.0001\n",
      "Current: 35\tTrain Acc: 0.50780000269413\tTrain Cost: 1.3277736455202103\tTest Acc: 0.4511249974370003\tTest Cost: 1.4733498126268387\tLR: 0.0001\n",
      "Current: 36\tTrain Acc: 0.5151999980211258\tTrain Cost: 1.3242350697517395\tTest Acc: 0.4498750001192093\tTest Cost: 1.481665512919426\tLR: 0.0001\n",
      "Current: 37\tTrain Acc: 0.5266000017523765\tTrain Cost: 1.3065266728401184\tTest Acc: 0.47837499920278787\tTest Cost: 1.4183969728648662\tLR: 0.0001\n",
      "Current: 38\tTrain Acc: 0.5268000009655952\tTrain Cost: 1.2994889295101166\tTest Acc: 0.47549999933689835\tTest Cost: 1.4366967707872391\tLR: 0.0001\n",
      "Current: 39\tTrain Acc: 0.5419999986886979\tTrain Cost: 1.2680833154916764\tTest Acc: 0.45424999929964543\tTest Cost: 1.4535461224615573\tLR: 0.0001\n",
      "Current: 40\tTrain Acc: 0.5443999996781349\tTrain Cost: 1.268057278394699\tTest Acc: 0.4634999979287386\tTest Cost: 1.4447992742061615\tLR: 0.0001\n",
      "Current: 41\tTrain Acc: 0.5366000011563301\tTrain Cost: 1.2684802210330963\tTest Acc: 0.4862499997019768\tTest Cost: 1.4162287466228007\tLR: 0.0001\n",
      "Current: 42\tTrain Acc: 0.5410000005364418\tTrain Cost: 1.2579102861881255\tTest Acc: 0.48362499959766864\tTest Cost: 1.4091505326330662\tLR: 0.0001\n",
      "Current: 43\tTrain Acc: 0.5568000018596649\tTrain Cost: 1.2418834471702576\tTest Acc: 0.49462500084191563\tTest Cost: 1.3897125199437141\tLR: 0.0001\n",
      "Current: 44\tTrain Acc: 0.5494000023603439\tTrain Cost: 1.2361153131723404\tTest Acc: 0.4693749977275729\tTest Cost: 1.4405418403446675\tLR: 0.0001\n",
      "Current: 45\tTrain Acc: 0.5560000005364418\tTrain Cost: 1.2273081982135772\tTest Acc: 0.4814999980852008\tTest Cost: 1.3944392289966345\tLR: 0.0001\n",
      "Current: 46\tTrain Acc: 0.5525999999046326\tTrain Cost: 1.2180385142564774\tTest Acc: 0.4887499984353781\tTest Cost: 1.3985800914466382\tLR: 0.0001\n",
      "Current: 47\tTrain Acc: 0.5547999984025955\tTrain Cost: 1.21089043200016\tTest Acc: 0.49249999932944777\tTest Cost: 1.4024583667516708\tLR: 0.0001\n",
      "Current: 48\tTrain Acc: 0.5742000019550324\tTrain Cost: 1.184877982735634\tTest Acc: 0.5014999998733402\tTest Cost: 1.372709186375141\tLR: 0.0001\n",
      "Current: 49\tTrain Acc: 0.5596000003814697\tTrain Cost: 1.209569030404091\tTest Acc: 0.4870000001974404\tTest Cost: 1.4191147424280643\tLR: 0.0001\n",
      "Current: 50\tTrain Acc: 0.569600002169609\tTrain Cost: 1.1898935800790786\tTest Acc: 0.48025000020861625\tTest Cost: 1.4198083698749542\tLR: 0.0001\n",
      "Current: 51\tTrain Acc: 0.560600001513958\tTrain Cost: 1.2105266612768173\tTest Acc: 0.49537499975413085\tTest Cost: 1.3826326534152031\tLR: 0.0001\n",
      "Current: 52\tTrain Acc: 0.5710000020265579\tTrain Cost: 1.1768442326784134\tTest Acc: 0.49312499882653354\tTest Cost: 1.3855288121849298\tLR: 0.0001\n",
      "Current: 53\tTrain Acc: 0.5714000022411346\tTrain Cost: 1.1722121453285217\tTest Acc: 0.5053749993443489\tTest Cost: 1.3520290892571212\tLR: 0.0001\n",
      "Current: 54\tTrain Acc: 0.5844000035524368\tTrain Cost: 1.1485845375061035\tTest Acc: 0.485124999564141\tTest Cost: 1.4061213284730911\tLR: 0.0001\n",
      "Current: 55\tTrain Acc: 0.5900000056624413\tTrain Cost: 1.1390219300985336\tTest Acc: 0.4913749981671572\tTest Cost: 1.3910585541278124\tLR: 0.0001\n",
      "Current: 56\tTrain Acc: 0.5956000038981437\tTrain Cost: 1.1289336782693864\tTest Acc: 0.5023750010877848\tTest Cost: 1.3603509359061718\tLR: 0.0001\n",
      "Current: 57\tTrain Acc: 0.5930000025033951\tTrain Cost: 1.1391110116243361\tTest Acc: 0.5032499989494681\tTest Cost: 1.3788435734808444\tLR: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 58\tTrain Acc: 0.6084000024199486\tTrain Cost: 1.1092459243535995\tTest Acc: 0.48750000055879356\tTest Cost: 1.412041464447975\tLR: 0.0001\n",
      "Current: 59\tTrain Acc: 0.5986000025272369\tTrain Cost: 1.1115005493164063\tTest Acc: 0.4879999991506338\tTest Cost: 1.4051990307867528\tLR: 0.0001\n",
      "Current: 60\tTrain Acc: 0.5922000029683113\tTrain Cost: 1.1298802465200424\tTest Acc: 0.4997500002384186\tTest Cost: 1.3842106632888318\tLR: 0.0001\n",
      "Current: 61\tTrain Acc: 0.622800005376339\tTrain Cost: 1.0724905800819398\tTest Acc: 0.492625000141561\tTest Cost: 1.4119059335440398\tLR: 0.0001\n",
      "Current: 62\tTrain Acc: 0.6112000024318696\tTrain Cost: 1.0903642469644546\tTest Acc: 0.5144999999552965\tTest Cost: 1.3471148263663053\tLR: 0.0001\n",
      "Current: 63\tTrain Acc: 0.6166000041365624\tTrain Cost: 1.0874566942453385\tTest Acc: 0.49587499890476466\tTest Cost: 1.4022948019206525\tLR: 0.0001\n",
      "Current: 64\tTrain Acc: 0.6282000070810319\tTrain Cost: 1.0632027864456177\tTest Acc: 0.4997500002384186\tTest Cost: 1.4031257651746274\tLR: 0.0001\n",
      "Current: 65\tTrain Acc: 0.6154000049829483\tTrain Cost: 1.0701404529809952\tTest Acc: 0.4918750001117587\tTest Cost: 1.413779553025961\tLR: 0.0001\n",
      "Current: 66\tTrain Acc: 0.6242000025510788\tTrain Cost: 1.041240918636322\tTest Acc: 0.5034999994561076\tTest Cost: 1.3695080183446406\tLR: 0.0001\n",
      "Current Iter : 67/100 batch : 3100/5000 acc : 0.58 cost : 1.17786696\r"
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "for iter in range(1,num_epoch+1):\n",
    "    for current_batch_index in range(0,len(x_train),batch_size):\n",
    "        current_data  = x_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost,grad_update],feed_dict={x:current_data,y:current_label,iter_num:iter})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_train)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        avg_lss_train = avg_lss_train + sess_results[1]\n",
    "    for current_batch_index in range(0,len(x_test), batch_size):\n",
    "        current_data  = x_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_test)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_test = avg_acc_test + sess_results[0]\n",
    "        avg_lss_test = avg_lss_test + sess_results[1]\n",
    "    # ======================== print reset ========================  \n",
    "    print(\"Current: \"+ str(iter) + \n",
    "          \"\\tTrain Acc: \"  + str(avg_acc_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTrain Cost: \" + str(avg_lss_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTest Acc: \"   + str(avg_acc_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tTest Cost: \"  + str(avg_lss_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tLR: \" + str(learning_rate) )\n",
    "    avg_acc_train   = 0 ; avg_acc_test  = 0 ; avg_lss_train = 0 ; avg_lss_test  = 0\n",
    "    x_train,y_train = shuffle(x_train,y_train); x_test,y_test   = shuffle(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Mean - in weight tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T01:33:31.827907Z",
     "start_time": "2019-06-18T01:30:15.180257Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 1\tTrain Acc: 0.19559999957680702\tTrain Cost: 2.1409799218177796\tTest Acc: 0.27762499884702263\tTest Cost: 1.9061641797423363\tLR: 0.0008\n",
      "Current: 2\tTrain Acc: 0.33500000163912774\tTrain Cost: 1.78861971616745\tTest Acc: 0.3726250001229346\tTest Cost: 1.6817388981580734\tLR: 0.0008\n",
      "Current: 3\tTrain Acc: 0.3965999999642372\tTrain Cost: 1.6291186547279357\tTest Acc: 0.4117499995045364\tTest Cost: 1.5833801381289958\tLR: 0.0008\n",
      "Current: 4\tTrain Acc: 0.4347999981045723\tTrain Cost: 1.5213914012908936\tTest Acc: 0.4377499992959201\tTest Cost: 1.51798485070467\tLR: 0.0008\n",
      "Current: 5\tTrain Acc: 0.4780000001192093\tTrain Cost: 1.4308626174926757\tTest Acc: 0.4556249985471368\tTest Cost: 1.4759912744164467\tLR: 0.0008\n",
      "Current: 6\tTrain Acc: 0.511599999666214\tTrain Cost: 1.3530434685945512\tTest Acc: 0.47649999912828206\tTest Cost: 1.4081824906170368\tLR: 0.0008\n",
      "Current: 7\tTrain Acc: 0.5293999999761582\tTrain Cost: 1.2989896112680435\tTest Acc: 0.48274999875575303\tTest Cost: 1.4063894897699356\tLR: 0.0008\n",
      "Current: 8\tTrain Acc: 0.5502000004053116\tTrain Cost: 1.2369221717119216\tTest Acc: 0.4973749993368983\tTest Cost: 1.3722591437399387\tLR: 0.0008\n",
      "Current: 9\tTrain Acc: 0.5585999986529351\tTrain Cost: 1.198109130859375\tTest Acc: 0.4938749995082617\tTest Cost: 1.3832597315311432\tLR: 0.0008\n",
      "Current: 10\tTrain Acc: 0.5854000025987625\tTrain Cost: 1.150710431933403\tTest Acc: 0.5147499971091747\tTest Cost: 1.334352659061551\tLR: 0.0008\n",
      "Current: 11\tTrain Acc: 0.597000002861023\tTrain Cost: 1.1155456137657165\tTest Acc: 0.5083749981597065\tTest Cost: 1.3523104198276996\tLR: 0.0008\n",
      "Current: 12\tTrain Acc: 0.6068000030517579\tTrain Cost: 1.086259176135063\tTest Acc: 0.5162500012665987\tTest Cost: 1.3319330289959908\tLR: 0.0008\n",
      "Current: 13\tTrain Acc: 0.6194000035524369\tTrain Cost: 1.0430400657653809\tTest Acc: 0.5240000002086163\tTest Cost: 1.3377283312380315\tLR: 0.0008\n",
      "Current: 14\tTrain Acc: 0.6194000029563904\tTrain Cost: 1.0398136472702026\tTest Acc: 0.5225000012665987\tTest Cost: 1.3285104002803565\tLR: 0.0008\n",
      "Current: 15\tTrain Acc: 0.6386000066995621\tTrain Cost: 0.9952584594488144\tTest Acc: 0.5328750003129243\tTest Cost: 1.3103054251521826\tLR: 0.0008\n",
      "Current: 16\tTrain Acc: 0.6452000039815903\tTrain Cost: 0.9669962984323501\tTest Acc: 0.5408750016242265\tTest Cost: 1.3002511892467736\tLR: 0.0008\n",
      "Current: 17\tTrain Acc: 0.6648000067472458\tTrain Cost: 0.9274542480707169\tTest Acc: 0.5448750022798776\tTest Cost: 1.2850014451891183\tLR: 0.0008\n",
      "Current: 18\tTrain Acc: 0.6740000060200692\tTrain Cost: 0.9154990565776825\tTest Acc: 0.5206250008195639\tTest Cost: 1.3414748456329106\tLR: 0.0008\n",
      "Current: 19\tTrain Acc: 0.6804000055789947\tTrain Cost: 0.8897434991598129\tTest Acc: 0.5491250013932586\tTest Cost: 1.2952396415174008\tLR: 0.0008\n",
      "Current: 20\tTrain Acc: 0.6917999994754791\tTrain Cost: 0.8510694199800491\tTest Acc: 0.5347500015050173\tTest Cost: 1.3263955142349004\tLR: 0.0008\n",
      "Current: 21\tTrain Acc: 0.6992000061273574\tTrain Cost: 0.8217468434572219\tTest Acc: 0.5432500045746564\tTest Cost: 1.3120289593935013\tLR: 0.0008\n",
      "Current: 22\tTrain Acc: 0.7014000031352043\tTrain Cost: 0.83437579870224\tTest Acc: 0.54825000166893\tTest Cost: 1.3042089898139237\tLR: 0.0008\n",
      "Current: 23\tTrain Acc: 0.7210000032186508\tTrain Cost: 0.7821512481570244\tTest Acc: 0.5452500009909272\tTest Cost: 1.3459082923829555\tLR: 0.0008\n",
      "Current: 24\tTrain Acc: 0.7334000033140182\tTrain Cost: 0.7515731623768807\tTest Acc: 0.5463749999180436\tTest Cost: 1.3495758660137653\tLR: 0.0008\n",
      "Current Iter : 25/30 batch : 4100/5000 acc : 0.78 cost : 0.70035964\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-c8ce8342c53d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_update\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter_num\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' cost : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "for iter in range(1,num_epoch+1):\n",
    "    for current_batch_index in range(0,len(x_train),batch_size):\n",
    "        current_data  = x_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_train[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost,grad_update],feed_dict={x:current_data,y:current_label,iter_num:iter})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_train)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        avg_lss_train = avg_lss_train + sess_results[1]\n",
    "    for current_batch_index in range(0,len(x_test), batch_size):\n",
    "        current_data  = x_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = y_test[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,cost],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : '+str(iter)+'/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(x_test)) + ' acc : '+str(sess_results[0]) + ' cost : '+str(sess_results[1])+ '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        avg_acc_test = avg_acc_test + sess_results[0]\n",
    "        avg_lss_test = avg_lss_test + sess_results[1]\n",
    "    # ======================== print reset ========================  \n",
    "    print(\"Current: \"+ str(iter) + \n",
    "          \"\\tTrain Acc: \"  + str(avg_acc_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTrain Cost: \" + str(avg_lss_train/(len(x_train)/batch_size)) + \n",
    "          \"\\tTest Acc: \"   + str(avg_acc_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tTest Cost: \"  + str(avg_lss_test/(len(x_test)/batch_size)) + \n",
    "          \"\\tLR: \" + str(learning_rate) )\n",
    "    avg_acc_train   = 0 ; avg_acc_test  = 0 ; avg_lss_train = 0 ; avg_lss_test  = 0\n",
    "    x_train,y_train = shuffle(x_train,y_train); x_test,y_test   = shuffle(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T00:12:01.633084Z",
     "start_time": "2019-06-05T00:12:01.531955Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T02:56:34.173614Z",
     "start_time": "2019-06-18T02:56:33.970944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[-0.9791961   1.3355689  -0.5717732   1.6240884   0.5671745 ]\n",
      " [ 0.3059743   0.3729131  -0.03954097 -0.54076743  0.5313867 ]]\n",
      "\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]]\n",
      "\n",
      "[[-0.          0.         -0.5717732   0.          0.5671745 ]\n",
      " [ 0.3059743   0.3729131  -0.03954097 -0.          0.        ]]\n",
      "\n",
      "[[-0.9791961   1.3355689  -0.          1.6240884   0.        ]\n",
      " [ 0.          0.         -0.         -0.54076743  0.5313867 ]]\n"
     ]
    }
   ],
   "source": [
    "# see \n",
    "temp = tf.random_normal([2,5],stddev=1,seed=4)\n",
    "\n",
    "dimension_shape = (1)\n",
    "temp_pos      = tf.where(temp>0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_pos_mean = tf.reduce_sum(temp_pos*temp,dimension_shape,keepdims=True)/tf.reduce_sum(temp_pos,dimension_shape,keepdims=True)\n",
    "\n",
    "print(\n",
    "tf.reduce_sum(temp_pos*temp,1,keepdims=True).shape\n",
    ")\n",
    "\n",
    "temp_neg      = tf.where(temp<0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_neg_mean = tf.reduce_sum(temp_neg*temp,dimension_shape,keepdims=True)/tf.reduce_sum(temp_neg,dimension_shape,keepdims=True)\n",
    "\n",
    "temp_between  = tf.where(temp_neg_mean<temp,tf.ones_like(temp),tf.zeros_like(temp)) * tf.where(temp_pos_mean>temp,tf.ones_like(temp),tf.zeros_like(temp)) \n",
    "\n",
    "temp1 = temp_between * temp\n",
    "temp2 = (1-temp_between) * temp\n",
    "\n",
    "all_of_data = sess.run([temp,temp_between,temp1,temp2])\n",
    "\n",
    "print(all_of_data[0])\n",
    "print()\n",
    "print(all_of_data[1])\n",
    "print()\n",
    "print(all_of_data[2])\n",
    "print()\n",
    "print(all_of_data[3])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T01:17:45.508045Z",
     "start_time": "2019-06-18T01:17:45.410305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9791961  1.3355689 -0.5717732  1.6240884  0.5671745]\n",
      "[0. 1. 0. 1. 1.]\n",
      "1.1756105\n",
      "[1. 0. 1. 0. 0.]\n",
      "-0.7754846\n",
      "[-0.         0.        -0.5717732  0.         0.5671745]\n",
      "[-0.9791961  1.3355689 -0.         1.6240884  0.       ]\n"
     ]
    }
   ],
   "source": [
    "# see \n",
    "temp = tf.random_normal([5],stddev=1,seed=4)\n",
    "\n",
    "temp_pos      = tf.where(temp>0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_pos_mean = tf.reduce_sum(temp_pos*temp)/tf.reduce_sum(temp_pos)\n",
    "\n",
    "temp_neg      = tf.where(temp<0,tf.ones_like(temp),tf.zeros_like(temp))\n",
    "temp_neg_mean = tf.reduce_sum(temp_neg*temp)/tf.reduce_sum(temp_neg)\n",
    "\n",
    "temp_between  = tf.where(temp_neg_mean<temp,tf.ones_like(temp),tf.zeros_like(temp)) * tf.where(temp_pos_mean>temp,tf.ones_like(temp),tf.zeros_like(temp)) \n",
    "\n",
    "temp1 = temp_between * temp\n",
    "temp2 = (1-temp_between) * temp\n",
    "\n",
    "all_of_data = sess.run([temp,\n",
    "                        temp_pos,temp_pos_mean,\n",
    "                        temp_neg,temp_neg_mean,temp1,temp2])\n",
    "\n",
    "print(all_of_data[0])\n",
    "\n",
    "print(all_of_data[1])\n",
    "print(all_of_data[2])\n",
    "print(all_of_data[3])\n",
    "\n",
    "print(all_of_data[4])\n",
    "print(all_of_data[5])\n",
    "print(all_of_data[6])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
