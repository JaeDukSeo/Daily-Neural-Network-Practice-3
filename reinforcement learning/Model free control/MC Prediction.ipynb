{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T00:55:58.605973Z",
     "start_time": "2019-03-21T00:55:57.241458Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from reference.lib.envs.blackjack import BlackjackEnv\n",
    "from reference.lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T00:56:00.437374Z",
     "start_time": "2019-03-21T00:56:00.431424Z"
    }
   },
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:25:12.235282Z",
     "start_time": "2019-03-21T01:25:12.225328Z"
    }
   },
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum   = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    # start the loop\n",
    "    for _ in range(num_episodes):\n",
    "        \n",
    "        # reset the state \n",
    "        observation = env.reset()\n",
    "        action_to_take = sample_policy(observation)\n",
    "        next_observation,reward,game_over,_ = env.step(action_to_take)\n",
    "        \n",
    "        if observation in returns_count: returns_count[observation] = returns_count[observation] + 1\n",
    "        else:                            returns_count[observation] = 1\n",
    "        \n",
    "        \n",
    "        current_state_to_estimate = observation[0]\n",
    "\n",
    "        if current_state_to_estimate in returns_count:\n",
    "            returns_count[current_state_to_estimate] = returns_count[current_state_to_estimate] + 1\n",
    "        else:\n",
    "            returns_count[current_state_to_estimate] = 1\n",
    "\n",
    "        action_to_take = sample_policy(observation)\n",
    "        observation,reward,game_over,_ = env.step(action_to_take)\n",
    "\n",
    "        while not game_over: \n",
    "            action_to_take = sample_policy(observation)\n",
    "            observation,reward,game_over,_ = env.step(action_to_take)\n",
    "\n",
    "        if current_state_to_estimate in returns_count:\n",
    "            returns_sum[current_state_to_estimate] = returns_sum[current_state_to_estimate] + reward\n",
    "        else:\n",
    "            returns_sum[current_state_to_estimate] = reward\n",
    "\n",
    "    for x in returns_sum.keys():\n",
    "        V[x] = returns_sum[x]/returns_count[x]\n",
    "\n",
    "    return V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:25:12.452110Z",
     "start_time": "2019-03-21T01:25:12.448639Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is > 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 20 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:25:13.254742Z",
     "start_time": "2019-03-21T01:25:12.817272Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-a8dbe4389e5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mV_10k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_10k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"10,000 Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mV_500k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_500k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"500,000 Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Daily-Neural-Network-Practice-3\\reinforcement learning\\reference\\lib\\plotting.py\u001b[0m in \u001b[0;36mplot_value_function\u001b[1;34m(V, title)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mPlots\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mfunction\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msurface\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmin_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mmax_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmin_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Daily-Neural-Network-Practice-3\\reinforcement learning\\reference\\lib\\plotting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mPlots\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mfunction\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msurface\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmin_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mmax_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmin_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "V_10k = mc_prediction(sample_policy, env, num_episodes=10000)\n",
    "plotting.plot_value_function(V_10k, title=\"10,000 Steps\")\n",
    "\n",
    "V_500k = mc_prediction(sample_policy, env, num_episodes=500000)\n",
    "plotting.plot_value_function(V_500k, title=\"500,000 Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:25:15.683988Z",
     "start_time": "2019-03-21T01:25:15.679051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {21: 0.8735294117647059,\n",
       "             17: -0.657051282051282,\n",
       "             13: -0.582089552238806,\n",
       "             12: -0.5525862068965517,\n",
       "             18: -0.6181015452538632,\n",
       "             16: -0.670490093847758,\n",
       "             14: -0.5636025998142989,\n",
       "             20: 0.6028636021100227,\n",
       "             15: -0.6030977734753146,\n",
       "             19: -0.6819923371647509})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:24:09.948347Z",
     "start_time": "2019-03-21T01:24:09.937423Z"
    }
   },
   "outputs": [],
   "source": [
    "returns_sum   = defaultdict(float)\n",
    "returns_count = defaultdict(float)\n",
    "\n",
    "# The final value function\n",
    "V = defaultdict(float)\n",
    "player,dealer,game_over = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    observation = env.reset()\n",
    "    current_state_to_estimate =observation[0]\n",
    "\n",
    "    if current_state_to_estimate in returns_count:\n",
    "        returns_count[current_state_to_estimate] = returns_count[current_state_to_estimate] + 1\n",
    "    else:\n",
    "        returns_count[current_state_to_estimate] = 1\n",
    "\n",
    "    action_to_take = sample_policy(observation)\n",
    "    observation,reward,game_over,_ = env.step(action_to_take)\n",
    "\n",
    "    while not game_over: \n",
    "        action_to_take = sample_policy(observation)\n",
    "        observation,reward,game_over,_ = env.step(action_to_take)\n",
    "\n",
    "    if current_state_to_estimate in returns_count:\n",
    "        returns_sum[current_state_to_estimate] = returns_sum[current_state_to_estimate] + reward\n",
    "    else:\n",
    "        returns_sum[current_state_to_estimate] = reward\n",
    "\n",
    "for x in returns_sum.keys():\n",
    "    V[x] = returns_sum[x]/returns_count[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:24:11.411987Z",
     "start_time": "2019-03-21T01:24:11.408006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([14, 12, 20, 18, 19, 16, 13, 17])\n",
      "dict_keys([14, 12, 20, 18, 19, 16, 13, 17])\n"
     ]
    }
   ],
   "source": [
    "print(returns_sum.keys())\n",
    "print(returns_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:24:43.473528Z",
     "start_time": "2019-03-21T01:24:43.467569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {14: -1.0,\n",
       "             12: -1.0,\n",
       "             20: 1.0,\n",
       "             18: -1.0,\n",
       "             19: -1.0,\n",
       "             16: 1.0,\n",
       "             13: -1.0,\n",
       "             17: -1.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:14:15.906149Z",
     "start_time": "2019-03-21T01:14:15.901173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117, 5, False), -1, True, {})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:04:26.018644Z",
     "start_time": "2019-03-21T01:04:26.010669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__  :  <class 'reference.lib.envs.blackjack.BlackjackEnv'>\n",
      "__delattr__  :  <method-wrapper '__delattr__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__dict__  :  {'action_space': Discrete(2), 'observation_space': Tuple(Discrete(32), Discrete(11), Discrete(2)), 'np_random': <mtrand.RandomState object at 0x000001B3C2F1D0D8>, 'natural': False, 'dealer': [10, 2], 'player': [6, 1], 'nA': 2}\n",
      "__dir__  :  <built-in method __dir__ of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__doc__  :  Simple blackjack environment\n",
      "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
      "    near as possible to 21 without going over.  They're playing against a fixed\n",
      "    dealer.\n",
      "    Face cards (Jack, Queen, King) have point value 10.\n",
      "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
      "    This game is placed with an infinite deck (or with replacement).\n",
      "    The game starts with each (player and dealer) having one face up and one\n",
      "    face down card.\n",
      "    The player can request additional cards (hit=1) until they decide to stop\n",
      "    (stick=0) or exceed 21 (bust).\n",
      "    After the player sticks, the dealer reveals their facedown card, and draws\n",
      "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
      "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
      "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
      "    drawing is 0, and losing is -1.\n",
      "    The observation of a 3-tuple of: the players current sum,\n",
      "    the dealer's one showing card (1-10 where 1 is ace),\n",
      "    and whether or not the player holds a usable ace (0 or 1).\n",
      "    This environment corresponds to the version of the blackjack problem\n",
      "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
      "    by Sutton and Barto (1998).\n",
      "    https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\n",
      "    \n",
      "__eq__  :  <method-wrapper '__eq__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__format__  :  <built-in method __format__ of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__ge__  :  <method-wrapper '__ge__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__getattribute__  :  <method-wrapper '__getattribute__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__gt__  :  <method-wrapper '__gt__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__hash__  :  <method-wrapper '__hash__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__init__  :  <bound method BlackjackEnv.__init__ of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "__init_subclass__  :  <built-in method __init_subclass__ of type object at 0x000001B3C08FA3D8>\n",
      "__le__  :  <method-wrapper '__le__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__lt__  :  <method-wrapper '__lt__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__module__  :  reference.lib.envs.blackjack\n",
      "__ne__  :  <method-wrapper '__ne__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__new__  :  <built-in method __new__ of type object at 0x000000005039B540>\n",
      "__reduce__  :  <built-in method __reduce__ of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__reduce_ex__  :  <built-in method __reduce_ex__ of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__repr__  :  <method-wrapper '__repr__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__setattr__  :  <method-wrapper '__setattr__' of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__sizeof__  :  <built-in method __sizeof__ of BlackjackEnv object at 0x000001B3C01B4E80>\n",
      "__str__  :  <bound method Env.__str__ of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "__subclasshook__  :  <built-in method __subclasshook__ of type object at 0x000001B3C08FA3D8>\n",
      "__weakref__  :  None\n",
      "_get_obs  :  <bound method BlackjackEnv._get_obs of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "_reset  :  <bound method BlackjackEnv._reset of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "_seed  :  <bound method BlackjackEnv._seed of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "_step  :  <bound method BlackjackEnv._step of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "action_space  :  Discrete(2)\n",
      "close  :  <bound method Env.close of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "dealer  :  [10, 2]\n",
      "metadata  :  {'render.modes': []}\n",
      "nA  :  2\n",
      "natural  :  False\n",
      "np_random  :  <mtrand.RandomState object at 0x000001B3C2F1D0D8>\n",
      "observation_space  :  Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "player  :  [6, 1]\n",
      "render  :  <bound method Env.render of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "reset  :  <bound method BlackjackEnv.reset of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "reward_range  :  (-inf, inf)\n",
      "seed  :  <bound method Env.seed of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "spec  :  None\n",
      "step  :  <bound method BlackjackEnv.step of <reference.lib.envs.blackjack.BlackjackEnv object at 0x000001B3C01B4E80>>\n",
      "unwrapped  :  <BlackjackEnv instance>\n"
     ]
    }
   ],
   "source": [
    "for x in dir(env):\n",
    "    temp = getattr(env,x)\n",
    "    print(x,' : ', temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
